{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients of all machine learning\n",
    "\n",
    "1. The model\n",
    "1. The cost function\n",
    "1. The optimizer\n",
    "1. Training loop\n",
    "1. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're building a linear regression model. This takes in a feature vector $x$, multiplies it by a vector of weights $W$ to get a single value, and adds a bias term $b$, to obtain the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Linear(num_features, 1)\n",
    "        # Linear implicitly defines its own variables - weights W and a bias term b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # this computes W*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the linear regression model\n",
    "# we'll use the Boston Housing dataset, which has 13 features\n",
    "num_features = 13\n",
    "model = LinearRegressor(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressor(\n",
       "  (model): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization also randomly initializes the weights and bias term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2593,  0.2445,  0.2290, -0.0083, -0.2726, -0.2167,  0.1122, -0.0568,\n",
       "          -0.1211, -0.0373,  0.1343, -0.0145, -0.1811]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1605], requires_grad=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cost function\n",
    "\n",
    "This is usually called the \"criterion\" in PyTorch contexts. For linear regression, this is the mean squared error. For many other applications, such as logistic regression, this is usually the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic optimizer is Stochastic Gradient Descent. However, a good default to choose is generally the Adam optimizer, so that's what we'll do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the train loop, we have to do the following:\n",
    "\n",
    "1. Zero out the gradients\n",
    "1. Take in the x's and y's, moving them to the GPU if necessary\n",
    "1. Run the model forward\n",
    "1. Calculate the loss\n",
    "1. Back-propagate the loss to calculate the deltas of the model parameters\n",
    "1. Update the parameters of the model\n",
    "\n",
    "The following lines of code show how these work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero out the gradients\n",
    "optimizer.zero_grad()\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  tensor([[0.7741, 0.2144, 0.7263, 0.4072, 0.3888, 0.0608, 0.1825, 0.9052, 0.0596,\n",
      "         0.5792, 0.7549, 0.3875, 0.9979]])\n",
      "Target:  tensor([[0.2877]])\n",
      "Output:  tensor([[-0.0888]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Run the model forward on some random data\n",
    "\n",
    "inputs = torch.rand(1, 13)\n",
    "target = torch.rand(1, 1)\n",
    "\n",
    "output = model(inputs)\n",
    "\n",
    "print(\"Inputs: \", inputs)\n",
    "print(\"Target: \", target)\n",
    "print(\"Output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1418, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "\n",
    "loss = criterion(output, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back-propagate the loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this for all the examples in the training set, and cycle through the training set some number of times. Each cycle through the training set is called an *epoch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the training data\n",
    "\n",
    "Before we actually do our training, we'll need real data instead of some randomized data! A standard pattern is to build `Datasets` and then `DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/01_boston_housing_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 13 are our features and the last column `medv` represents the dependent (or target) variable that we are trying to predict: the median value of houses in this area.\n",
    "\n",
    "We start by defining a `DataSet` that will yield `(x, y)` pairs that the training loop can read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[:num_features]].astype(np.float32).to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonHousingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        # these two are optional\n",
    "        self.feature_names = list(df.columns[:num_features])\n",
    "        self.target_name = df.columns[num_features]\n",
    "\n",
    "        # these two are not, but you can name them anything you like\n",
    "        # as long as __getitem__ correctly returns the i'th (X, y) pair\n",
    "        # and __len__ returns the size of the dataset.\n",
    "        self.X = df[df.columns[:num_features]].astype(np.float32).to_numpy()\n",
    "        self.y = df['medv'].astype(np.float32).to_numpy()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return X, np.array([y])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "        6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "        4.980e+00], dtype=float32), array([24.], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "num_train_examples = math.floor(len(df) * train_ratio)\n",
    "\n",
    "train_ds = BostonHousingDataset(df[:num_train_examples])\n",
    "valid_ds = BostonHousingDataset(df[num_train_examples:])\n",
    "\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(len(valid_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that we've set the dataset up correctly, let's try running the model forward and getting a prediction from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.3728])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model.forward(torch.from_numpy(train_ds[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the actual y:\n",
    "train_ds[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since we haven't trained anything yet, this is still using the randomly initialized set of weights `W` and bias `b`.\n",
    "\n",
    "Here's an example of running the model forward on multiple rows in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.3728],\n",
      "        [-4.7175],\n",
      "        [-5.9226]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model.forward(torch.from_numpy(train_ds[:3][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24. , 21.6, 34.7]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the actual y vector:\n",
    "train_ds[:3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset set up, we'll go further and create a couple of `DataLoader`s, one for training and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.DataLoader(train_ds,\n",
    "                                    batch_size=10, shuffle=True)\n",
    "valid = torch.utils.data.DataLoader(valid_ds,\n",
    "                                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 36\n",
      "Number of validation batches: 16\n"
     ]
    }
   ],
   "source": [
    "# Number of batches in training and testing:\n",
    "print(\"Number of training batches:\", len(train))\n",
    "print(\"Number of validation batches:\", len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, RMSE: 8.402\n",
      "Epoch: 20, RMSE: 7.493\n",
      "Epoch: 30, RMSE: 6.980\n",
      "Epoch: 40, RMSE: 6.653\n",
      "Epoch: 50, RMSE: 6.603\n",
      "Epoch: 60, RMSE: 6.405\n",
      "Epoch: 70, RMSE: 6.180\n",
      "Epoch: 80, RMSE: 6.180\n",
      "Epoch: 90, RMSE: 6.044\n",
      "Epoch: 100, RMSE: 5.886\n",
      "Epoch: 110, RMSE: 5.900\n",
      "Epoch: 120, RMSE: 5.812\n",
      "Epoch: 130, RMSE: 5.746\n",
      "Epoch: 140, RMSE: 5.617\n",
      "Epoch: 150, RMSE: 5.641\n",
      "Epoch: 160, RMSE: 5.504\n",
      "Epoch: 170, RMSE: 5.590\n",
      "Epoch: 180, RMSE: 5.557\n",
      "Epoch: 190, RMSE: 5.427\n",
      "Epoch: 200, RMSE: 5.325\n",
      "Epoch: 210, RMSE: 5.311\n",
      "Epoch: 220, RMSE: 5.298\n",
      "Epoch: 230, RMSE: 5.229\n",
      "Epoch: 240, RMSE: 5.270\n",
      "Epoch: 250, RMSE: 5.148\n",
      "Epoch: 260, RMSE: 5.163\n",
      "Epoch: 270, RMSE: 5.067\n",
      "Epoch: 280, RMSE: 5.028\n",
      "Epoch: 290, RMSE: 4.974\n",
      "Epoch: 300, RMSE: 4.955\n",
      "Epoch: 310, RMSE: 4.934\n",
      "Epoch: 320, RMSE: 4.849\n",
      "Epoch: 330, RMSE: 4.944\n",
      "Epoch: 340, RMSE: 4.844\n",
      "Epoch: 350, RMSE: 4.862\n",
      "Epoch: 360, RMSE: 4.855\n",
      "Epoch: 370, RMSE: 4.722\n",
      "Epoch: 380, RMSE: 4.735\n",
      "Epoch: 390, RMSE: 4.660\n",
      "Epoch: 400, RMSE: 4.693\n",
      "Epoch: 410, RMSE: 4.600\n",
      "Epoch: 420, RMSE: 4.649\n",
      "Epoch: 430, RMSE: 4.525\n",
      "Epoch: 440, RMSE: 4.501\n",
      "Epoch: 450, RMSE: 4.498\n",
      "Epoch: 460, RMSE: 4.515\n",
      "Epoch: 470, RMSE: 4.417\n",
      "Epoch: 480, RMSE: 4.385\n",
      "Epoch: 490, RMSE: 4.445\n",
      "Epoch: 500, RMSE: 4.398\n",
      "Epoch: 510, RMSE: 4.388\n",
      "Epoch: 520, RMSE: 4.495\n",
      "Epoch: 530, RMSE: 4.388\n",
      "Epoch: 540, RMSE: 4.295\n",
      "Epoch: 550, RMSE: 4.271\n",
      "Epoch: 560, RMSE: 4.232\n",
      "Epoch: 570, RMSE: 4.229\n",
      "Epoch: 580, RMSE: 4.216\n",
      "Epoch: 590, RMSE: 4.208\n",
      "Epoch: 600, RMSE: 4.158\n",
      "Epoch: 610, RMSE: 4.130\n",
      "Epoch: 620, RMSE: 4.248\n",
      "Epoch: 630, RMSE: 4.248\n",
      "Epoch: 640, RMSE: 4.035\n",
      "Epoch: 650, RMSE: 4.052\n",
      "Epoch: 660, RMSE: 4.020\n",
      "Epoch: 670, RMSE: 4.038\n",
      "Epoch: 680, RMSE: 4.027\n",
      "Epoch: 690, RMSE: 4.030\n",
      "Epoch: 700, RMSE: 4.002\n",
      "Epoch: 710, RMSE: 3.928\n",
      "Epoch: 720, RMSE: 3.901\n",
      "Epoch: 730, RMSE: 3.920\n",
      "Epoch: 740, RMSE: 3.921\n",
      "Epoch: 750, RMSE: 3.989\n",
      "Epoch: 760, RMSE: 3.827\n",
      "Epoch: 770, RMSE: 3.922\n",
      "Epoch: 780, RMSE: 3.905\n",
      "Epoch: 790, RMSE: 3.773\n",
      "Epoch: 800, RMSE: 3.914\n",
      "Epoch: 810, RMSE: 3.957\n",
      "Epoch: 820, RMSE: 3.778\n",
      "Epoch: 830, RMSE: 3.747\n",
      "Epoch: 840, RMSE: 3.709\n",
      "Epoch: 850, RMSE: 3.809\n",
      "Epoch: 860, RMSE: 3.823\n",
      "Epoch: 870, RMSE: 3.635\n",
      "Epoch: 880, RMSE: 3.713\n",
      "Epoch: 890, RMSE: 3.737\n",
      "Epoch: 900, RMSE: 3.742\n",
      "Epoch: 910, RMSE: 3.656\n",
      "Epoch: 920, RMSE: 3.632\n",
      "Epoch: 930, RMSE: 3.630\n",
      "Epoch: 940, RMSE: 3.746\n",
      "Epoch: 950, RMSE: 3.619\n",
      "Epoch: 960, RMSE: 3.571\n",
      "Epoch: 970, RMSE: 3.587\n",
      "Epoch: 980, RMSE: 3.626\n",
      "Epoch: 990, RMSE: 3.647\n",
      "Epoch: 1000, RMSE: 3.517\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "\n",
    "# if you have a GPU, it's probably better to do this there\n",
    "# you'll need to send both your model and the X/y data to the GPU (see below)\n",
    "# otherwise, this code will execute on CPU\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "model.train()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    for X_batch, y_batch in train:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_batch = X_batch.to('cuda')\n",
    "            y_batch = y_batch.to('cuda')\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    epoch_loss = np.sqrt(np.mean(epoch_losses))\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'Epoch: {i+1}, RMSE: {epoch_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is almost certainly over-fitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of linear regression, the criterion we optimize for -- mean square error -- is usually the same as the metric that we actually use to evaluate the system. As we move to logistic regression-like tasks, these will diverge.\n",
    "\n",
    "Because our loss criterion and our evaluation metric are the same, the following should seem like old hat. Nevertheless, let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressor(\n",
       "  (model): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if it's on the GPU, bring the model back to the CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# set our model to evaluation mode\n",
    "# for this particular model, this won't actually change anything.\n",
    "# in other models where the training and eval loop operate differently\n",
    "# (for example, if you're using dropout), this is absolutely necessary!\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  20.990253\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # don't do backprop\n",
    "    mses = list()\n",
    "    for X_batch, y_batch in valid:\n",
    "        y_pred = model(X_batch)\n",
    "        mse = criterion(y_pred, y_batch)\n",
    "        mses.append(mse)\n",
    "    print(\"RMSE: \", np.sqrt(np.mean(mses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RMSE is a lot higher than the training error, suggesting that we've overfit to the training set. There are some ways to fix this:\n",
    "\n",
    "* Adding regularization\n",
    "* Early stopping\n",
    "\n",
    "We'll implement these in future notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters you can modify:\n",
    "\n",
    "The code above sets some hyperparameters that you can try playing with:\n",
    "\n",
    "* Model:\n",
    "  * Initialization strategy, e.g. Xavier\n",
    "* Optimizer:\n",
    "  * Which optimizer to use\n",
    "  * Parameters of the optimizer, such as the learning rate\n",
    "* Training loop:\n",
    "  * Batch size\n",
    "  * Number of epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
